{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab Utils Demo - Comprehensive Example\n",
    "\n",
    "This notebook demonstrates all features of the lab_utils package for creating interactive Jupyter lab exercises."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Installation\n",
    "\n",
    "First, let's install the lab_utils package:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install the package (run this if not already installed)\n",
    "# !pip install -e lab_utils/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import all components\n",
    "from lab_utils import *\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Progress Tracking Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a progress tracker\n",
    "steps = [\n",
    "    \"Setup Environment\",\n",
    "    \"Load Data\",\n",
    "    \"Data Exploration\",\n",
    "    \"Data Cleaning\",\n",
    "    \"Feature Engineering\",\n",
    "    \"Model Training\",\n",
    "    \"Evaluation\"\n",
    "]\n",
    "\n",
    "progress = LabProgress(steps, lab_name=\"Machine Learning Lab\", persist=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mark first step as complete\n",
    "progress.mark_done(\"Setup Environment\", score=100, notes=\"All libraries imported successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show partial progress\n",
    "progress.mark_partial(\"Load Data\", 0.5, notes=\"Loading in progress...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete the data loading\n",
    "progress.mark_done(\"Load Data\", score=95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show detailed progress\n",
    "progress.display_progress(detailed=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get statistics\n",
    "print(f\"Completion Rate: {progress.get_completion_rate():.1f}%\")\n",
    "print(f\"Average Score: {progress.get_average_score():.1f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Display Utilities Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic message types\n",
    "show_info(\"This is an informational message\", title=\"Information\")\n",
    "show_success(\"Great job! You've completed this section\")\n",
    "show_warning(\"Be careful with the next step\", title=\"Important\")\n",
    "show_error(\"This is what an error looks like\", title=\"Error Example\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code display\n",
    "show_code(\"\"\"\n",
    "# Example function\n",
    "def process_data(df):\n",
    "    # Remove missing values\n",
    "    df = df.dropna()\n",
    "    \n",
    "    # Normalize numeric columns\n",
    "    numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
    "    df[numeric_cols] = (df[numeric_cols] - df[numeric_cols].mean()) / df[numeric_cols].std()\n",
    "    \n",
    "    return df\n",
    "\"\"\", title=\"Data Processing Function\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hints (collapsible)\n",
    "show_hint(\"Consider using pandas dropna() method to handle missing values\")\n",
    "show_hint(\"You might want to check the data types before normalization\", title=\"Pro Tip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Progress bars\n",
    "show_progress_bar(3, 7, label=\"Lab Progress\", color=\"#2196F3\")\n",
    "show_progress_bar(75, 100, label=\"Data Processing\", color=\"#FF9800\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# JSON display\n",
    "results = {\n",
    "    \"accuracy\": 0.95,\n",
    "    \"precision\": 0.93,\n",
    "    \"recall\": 0.97,\n",
    "    \"f1_score\": 0.95,\n",
    "    \"confusion_matrix\": [[85, 5], [3, 92]]\n",
    "}\n",
    "\n",
    "show_json(results, title=\"Model Performance Metrics\")\n",
    "show_json(results, title=\"Click to see metrics\", collapsed=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tables\n",
    "show_table(\n",
    "    headers=[\"Algorithm\", \"Accuracy\", \"Training Time\", \"Complexity\"],\n",
    "    rows=[\n",
    "        [\"Random Forest\", \"0.95\", \"2.3s\", \"O(n log n)\"],\n",
    "        [\"SVM\", \"0.93\", \"5.1s\", \"O(nÂ²)\"],\n",
    "        [\"Neural Network\", \"0.97\", \"15.2s\", \"O(n)\"],\n",
    "        [\"Logistic Regression\", \"0.89\", \"0.8s\", \"O(n)\"]\n",
    "    ],\n",
    "    title=\"Algorithm Comparison\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checklists\n",
    "show_checklist({\n",
    "    \"Import libraries\": True,\n",
    "    \"Load dataset\": True,\n",
    "    \"Explore data\": True,\n",
    "    \"Handle missing values\": False,\n",
    "    \"Feature engineering\": False,\n",
    "    \"Train model\": False,\n",
    "    \"Evaluate performance\": False\n",
    "}, title=\"Data Science Workflow\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tabs\n",
    "show_tabs({\n",
    "    \"Instructions\": \"\"\"\n",
    "    <h4>Task: Data Preprocessing</h4>\n",
    "    <ol>\n",
    "        <li>Load the dataset using pandas</li>\n",
    "        <li>Check for missing values</li>\n",
    "        <li>Handle missing data appropriately</li>\n",
    "        <li>Normalize numeric features</li>\n",
    "    </ol>\n",
    "    \"\"\",\n",
    "    \"Hints\": \"\"\"\n",
    "    <ul>\n",
    "        <li>Use <code>df.isnull().sum()</code> to check missing values</li>\n",
    "        <li>Consider <code>fillna()</code> or <code>dropna()</code></li>\n",
    "        <li>StandardScaler from sklearn can help with normalization</li>\n",
    "    </ul>\n",
    "    \"\"\",\n",
    "    \"Resources\": \"\"\"\n",
    "    <p>Useful links:</p>\n",
    "    <ul>\n",
    "        <li><a href=\"#\">Pandas Documentation</a></li>\n",
    "        <li><a href=\"#\">Data Preprocessing Guide</a></li>\n",
    "        <li><a href=\"#\">Feature Scaling Explained</a></li>\n",
    "    </ul>\n",
    "    \"\"\"\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Validation Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a validator linked to our progress tracker\n",
    "validator = LabValidator(progress_tracker=progress)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Create some data for validation\n",
    "df = pd.DataFrame({\n",
    "    'id': range(100),\n",
    "    'value': np.random.randn(100),\n",
    "    'category': np.random.choice(['A', 'B', 'C'], 100)\n",
    "})\n",
    "\n",
    "# Create a function to validate\n",
    "def preprocess_data(data):\n",
    "    return data.dropna()\n",
    "\n",
    "# Create an embedding\n",
    "embedding = np.random.randn(384).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variable validation\n",
    "validator.validate_variable_exists('df', globals(), expected_type=pd.DataFrame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function validation\n",
    "validator.validate_function_exists('preprocess_data', globals(), expected_params=['data'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataFrame validation\n",
    "validator.validate_dataframe(\n",
    "    df,\n",
    "    expected_shape=(100, 3),\n",
    "    expected_columns=['id', 'value', 'category']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embedding validation\n",
    "validator.check_embedding_shape(embedding, expected_dim=384)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Value in dataframe\n",
    "validator.assert_in_dataframe(df, 'category', 'A')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output validation\n",
    "result = 42\n",
    "validator.validate_output(result, expected=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Range validation\n",
    "accuracy = 0.95\n",
    "validator.validate_range(accuracy, min_val=0.0, max_val=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom validation with automatic progress update\n",
    "validator.validate_and_mark_complete(\n",
    "    \"Data Exploration\",\n",
    "    condition=(len(df) > 0 and 'value' in df.columns),\n",
    "    success_msg=\"Data exploration completed successfully!\",\n",
    "    failure_msg=\"Please load and explore the data first\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Integrated Lab Exercise Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset for a complete example\n",
    "clear()\n",
    "\n",
    "# Create a new lab\n",
    "lab_steps = [\"Load Data\", \"Clean Data\", \"Analyze Data\", \"Create Visualization\"]\n",
    "lab_progress = LabProgress(lab_steps, lab_name=\"Data Analysis Workshop\")\n",
    "lab_validator = LabValidator(progress_tracker=lab_progress)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Load Data\n",
    "show_info(\"Let's start by creating a sample dataset\", title=\"Step 1: Load Data\")\n",
    "show_code(\"\"\"\n",
    "# Create a sample dataset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "np.random.seed(42)\n",
    "sample_data = pd.DataFrame({\n",
    "    'id': range(1, 101),\n",
    "    'sales': np.random.randint(100, 1000, 100),\n",
    "    'region': np.random.choice(['North', 'South', 'East', 'West'], 100),\n",
    "    'satisfaction': np.random.uniform(1, 5, 100)\n",
    "})\n",
    "\n",
    "# Add some missing values\n",
    "sample_data.loc[sample_data.sample(10).index, 'sales'] = np.nan\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Student work area\n",
    "# TODO: Create the sample dataset as shown above\n",
    "\n",
    "# Your code here...\n",
    "np.random.seed(42)\n",
    "sample_data = pd.DataFrame({\n",
    "    'id': range(1, 101),\n",
    "    'sales': np.random.randint(100, 1000, 100),\n",
    "    'region': np.random.choice(['North', 'South', 'East', 'West'], 100),\n",
    "    'satisfaction': np.random.uniform(1, 5, 100)\n",
    "})\n",
    "sample_data.loc[sample_data.sample(10).index, 'sales'] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate step 1\n",
    "if 'sample_data' in globals():\n",
    "    validation_passed = lab_validator.validate_dataframe(\n",
    "        sample_data,\n",
    "        expected_columns=['id', 'sales', 'region', 'satisfaction']\n",
    "    )\n",
    "    if validation_passed:\n",
    "        lab_progress.mark_done(\"Load Data\", score=100)\n",
    "        show_success(\"Excellent! Data loaded successfully\")\n",
    "else:\n",
    "    show_error(\"Please create the sample_data DataFrame\")\n",
    "    show_hint(\"Copy the code from the example above\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Clean Data\n",
    "show_info(\"Now let's clean the data by handling missing values\", title=\"Step 2: Clean Data\")\n",
    "show_checklist({\n",
    "    \"Check for missing values\": False,\n",
    "    \"Decide on strategy (drop/fill)\": False,\n",
    "    \"Apply the strategy\": False,\n",
    "    \"Verify no missing values remain\": False\n",
    "}, title=\"Data Cleaning Tasks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Student work area\n",
    "# TODO: Clean the data\n",
    "\n",
    "# Check missing values\n",
    "print(\"Missing values:\")\n",
    "print(sample_data.isnull().sum())\n",
    "\n",
    "# Clean the data (example: fill with mean)\n",
    "cleaned_data = sample_data.copy()\n",
    "cleaned_data['sales'].fillna(cleaned_data['sales'].mean(), inplace=True)\n",
    "\n",
    "print(\"\\nAfter cleaning:\")\n",
    "print(cleaned_data.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate step 2\n",
    "if 'cleaned_data' in globals():\n",
    "    has_no_nulls = cleaned_data.isnull().sum().sum() == 0\n",
    "    lab_validator.validate_and_mark_complete(\n",
    "        \"Clean Data\",\n",
    "        condition=has_no_nulls,\n",
    "        success_msg=\"Data cleaned successfully! No missing values remain.\",\n",
    "        failure_msg=\"There are still missing values in the data\"\n",
    "    )\n",
    "else:\n",
    "    show_warning(\"Please create the cleaned_data variable\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Progress summary\n",
    "show_info(f\"Current completion: {lab_progress.get_completion_rate():.0f}%\", \n",
    "          title=\"Progress Update\")\n",
    "\n",
    "# Export report\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(lab_progress.export_report())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Advanced Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create step-specific validators\n",
    "validate_analysis = lab_validator.create_step_validator(\"Analyze Data\")\n",
    "\n",
    "# Use it for custom validation\n",
    "analysis_complete = True  # This would be your actual condition\n",
    "validate_analysis(\n",
    "    analysis_complete,\n",
    "    success_msg=\"Analysis completed with insights!\",\n",
    "    failure_msg=\"Please complete the data analysis first\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# String pattern validation\n",
    "email = \"student@example.com\"\n",
    "lab_validator.validate_string_pattern(\n",
    "    email,\n",
    "    pattern=r'^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}$',\n",
    "    description=\"email format\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List validation\n",
    "scores = [85, 92, 78, 95, 88]\n",
    "lab_validator.validate_list_items(\n",
    "    scores,\n",
    "    validator_func=lambda x: 0 <= x <= 100,\n",
    "    description=\"score validation (0-100)\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final summary with all features\n",
    "clear()\n",
    "show_success(\"Lab Utils Demo Complete!\", title=\"Congratulations\")\n",
    "lab_progress.display_progress(detailed=True)\n",
    "\n",
    "show_tabs({\n",
    "    \"Summary\": f\"\"\"\n",
    "    <h4>You've learned about:</h4>\n",
    "    <ul>\n",
    "        <li>Progress tracking with persistence</li>\n",
    "        <li>Rich display utilities</li>\n",
    "        <li>Comprehensive validation methods</li>\n",
    "        <li>Integration between components</li>\n",
    "    </ul>\n",
    "    <p>Completion Rate: {lab_progress.get_completion_rate():.1f}%</p>\n",
    "    \"\"\",\n",
    "    \"Next Steps\": \"\"\"\n",
    "    <h4>Try these ideas:</h4>\n",
    "    <ol>\n",
    "        <li>Create your own lab exercise</li>\n",
    "        <li>Add custom validation functions</li>\n",
    "        <li>Experiment with different display styles</li>\n",
    "        <li>Build a complete tutorial</li>\n",
    "    </ol>\n",
    "    \"\"\",\n",
    "    \"Documentation\": \"\"\"\n",
    "    <p>For more information, see the README.md file in the lab_utils package.</p>\n",
    "    <p>Happy teaching and learning! ð</p>\n",
    "    \"\"\"\n",
    "})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}