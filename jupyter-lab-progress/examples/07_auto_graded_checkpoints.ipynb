{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Auto-Graded Checkpoints Demo\n\nThis notebook demonstrates the new auto-grading functionality that provides automatic scoring with weighted rules.\n\n**Note:** If you encounter `AttributeError` about missing methods like `add_scoring_rule`, please restart the Jupyter kernel (Kernel â†’ Restart) to ensure the latest version of the module is loaded."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Import and reload to ensure we have the latest version\nimport importlib\nimport sys\n\n# Remove any cached modules\nmodules_to_remove = [key for key in sys.modules.keys() if key.startswith('jupyter_lab_progress')]\nfor module in modules_to_remove:\n    del sys.modules[module]\n\n# Now import fresh\nfrom jupyter_lab_progress import LabProgress, LabValidator, show_info\nimport pandas as pd\nimport numpy as np\n\nshow_info(\"Modules reloaded to ensure latest version with auto-grading is used!\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting Up Auto-Graded Lab\n",
    "\n",
    "Let's create a lab that will be automatically graded based on various criteria."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Create progress tracker and validator\nprogress = LabProgress(\n    steps=[\n        \"Import Libraries\",\n        \"Load Data\",\n        \"Data Preprocessing\",\n        \"Feature Engineering\",\n        \"Model Training\"\n    ],\n    lab_name=\"Machine Learning Lab\"\n)\n\nvalidator = LabValidator(progress_tracker=progress)\n\n# Check if auto-grading functionality is available\nshow_info(\"Checking auto-grading functionality...\")\nvalidator.check_auto_grading_available()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding Scoring Rules\n",
    "\n",
    "Define weighted scoring rules that will automatically grade the lab work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add scoring rule for library imports\n",
    "validator.add_scoring_rule(\n",
    "    name=\"Libraries Imported\",\n",
    "    weight=0.1,  # 10% of total grade\n",
    "    checker={\n",
    "        'type': 'variable_exists',\n",
    "        'var_name': 'pd',\n",
    "        'expected_type': type(pd)\n",
    "    },\n",
    "    description=\"Check if pandas is imported as pd\"\n",
    ")\n",
    "\n",
    "# Add scoring rule for data loading\n",
    "def check_data_loaded(globals_dict):\n",
    "    \"\"\"Custom checker function that returns (success, score)\"\"\"\n",
    "    if 'df' not in globals_dict:\n",
    "        return False, 0\n",
    "    \n",
    "    df = globals_dict['df']\n",
    "    if not isinstance(df, pd.DataFrame):\n",
    "        return False, 0\n",
    "    \n",
    "    # Partial credit based on data size\n",
    "    if len(df) >= 1000:\n",
    "        return True, 100\n",
    "    elif len(df) >= 500:\n",
    "        return True, 80\n",
    "    elif len(df) >= 100:\n",
    "        return True, 60\n",
    "    else:\n",
    "        return True, 40\n",
    "\n",
    "validator.add_scoring_rule(\n",
    "    name=\"Data Loaded\",\n",
    "    weight=0.2,  # 20% of total grade\n",
    "    checker=check_data_loaded,\n",
    "    description=\"Check if DataFrame 'df' is loaded with sufficient data\",\n",
    "    partial_credit=True\n",
    ")\n",
    "\n",
    "# Add scoring rule for preprocessing\n",
    "def check_preprocessing(globals_dict):\n",
    "    \"\"\"Check if data is properly preprocessed\"\"\"\n",
    "    if 'df' not in globals_dict:\n",
    "        return False, 0\n",
    "    \n",
    "    df = globals_dict['df']\n",
    "    score = 0\n",
    "    \n",
    "    # Check for no missing values (50 points)\n",
    "    if df.isnull().sum().sum() == 0:\n",
    "        score += 50\n",
    "    \n",
    "    # Check if numeric columns exist (50 points)\n",
    "    if len(df.select_dtypes(include=[np.number]).columns) > 0:\n",
    "        score += 50\n",
    "    \n",
    "    return score > 0, score\n",
    "\n",
    "validator.add_scoring_rule(\n",
    "    name=\"Data Preprocessing\",\n",
    "    weight=0.25,  # 25% of total grade\n",
    "    checker=check_preprocessing,\n",
    "    description=\"Check data cleaning and preprocessing steps\"\n",
    ")\n",
    "\n",
    "# Add scoring rule for feature engineering\n",
    "validator.add_scoring_rule(\n",
    "    name=\"Feature Engineering\",\n",
    "    weight=0.25,  # 25% of total grade\n",
    "    checker={\n",
    "        'type': 'variable_exists',\n",
    "        'var_name': 'features',\n",
    "        'expected_type': (list, np.ndarray, pd.DataFrame)\n",
    "    },\n",
    "    description=\"Check if features variable is created\"\n",
    ")\n",
    "\n",
    "# Add scoring rule for model\n",
    "def check_model(globals_dict):\n",
    "    \"\"\"Check if a model is trained and has expected methods\"\"\"\n",
    "    if 'model' not in globals_dict:\n",
    "        return False, 0\n",
    "    \n",
    "    model = globals_dict['model']\n",
    "    score = 0\n",
    "    \n",
    "    # Check for fit method (50 points)\n",
    "    if hasattr(model, 'fit'):\n",
    "        score += 50\n",
    "    \n",
    "    # Check for predict method (50 points)\n",
    "    if hasattr(model, 'predict'):\n",
    "        score += 50\n",
    "    \n",
    "    return score > 0, score\n",
    "\n",
    "validator.add_scoring_rule(\n",
    "    name=\"Model Training\",\n",
    "    weight=0.2,  # 20% of total grade\n",
    "    checker=check_model,\n",
    "    description=\"Check if a machine learning model is properly instantiated\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Student Work Simulation\n",
    "\n",
    "Let's simulate a student completing the lab tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "progress.mark_done(\"Import Libraries\")\n",
    "show_info(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Load data (creating sample data for demo)\n",
    "np.random.seed(42)\n",
    "df = pd.DataFrame({\n",
    "    'feature1': np.random.randn(1500),\n",
    "    'feature2': np.random.randn(1500),\n",
    "    'feature3': np.random.choice(['A', 'B', 'C'], 1500),\n",
    "    'target': np.random.choice([0, 1], 1500)\n",
    "})\n",
    "\n",
    "# Add some missing values to demonstrate preprocessing\n",
    "df.loc[df.sample(50).index, 'feature1'] = np.nan\n",
    "\n",
    "progress.mark_done(\"Load Data\")\n",
    "show_info(f\"Data loaded: {len(df)} rows, {len(df.columns)} columns\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run auto-grading at this point (partial completion)\n",
    "show_info(\"Running auto-grading on current progress...\")\n",
    "results = validator.run_auto_grading(globals())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Data preprocessing\n",
    "# Fill missing values\n",
    "df['feature1'].fillna(df['feature1'].mean(), inplace=True)\n",
    "\n",
    "# Convert categorical to numeric\n",
    "df = pd.get_dummies(df, columns=['feature3'])\n",
    "\n",
    "progress.mark_done(\"Data Preprocessing\")\n",
    "show_info(\"Data preprocessing completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Feature engineering\n",
    "features = df.drop('target', axis=1)\n",
    "target = df['target']\n",
    "\n",
    "# Create additional engineered features\n",
    "features['feature1_squared'] = features['feature1'] ** 2\n",
    "features['feature_interaction'] = features['feature1'] * features['feature2']\n",
    "\n",
    "progress.mark_done(\"Feature Engineering\")\n",
    "show_info(f\"Features created: {features.shape[1]} features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Model training\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    features, target, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "progress.mark_done(\"Model Training\")\n",
    "show_info(\"Model trained successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Auto-Grading\n",
    "\n",
    "Run the complete auto-grading to see the final score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run final auto-grading\n",
    "final_results = validator.run_auto_grading(globals())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export Grading Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate text report\n",
    "report = validator.export_grading_report()\n",
    "print(\"\\nText Report:\\n\")\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View validation history\n",
    "history_df = validator.get_validation_history_df()\n",
    "show_info(f\"Total validations performed: {len(history_df)}\")\n",
    "history_df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Scoring Functions\n",
    "\n",
    "You can create complex scoring functions that check multiple criteria."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of a comprehensive scoring function\n",
    "def comprehensive_model_check(globals_dict):\n",
    "    \"\"\"Advanced model validation with multiple criteria\"\"\"\n",
    "    score = 0\n",
    "    feedback = []\n",
    "    \n",
    "    # Check if model exists (20 points)\n",
    "    if 'model' not in globals_dict:\n",
    "        return False, 0\n",
    "    \n",
    "    model = globals_dict['model']\n",
    "    score += 20\n",
    "    \n",
    "    # Check if it's a sklearn model (20 points)\n",
    "    if hasattr(model, 'get_params'):\n",
    "        score += 20\n",
    "        feedback.append(\"âœ“ Valid sklearn model\")\n",
    "    \n",
    "    # Check if model is fitted (30 points)\n",
    "    if hasattr(model, 'n_features_in_'):\n",
    "        score += 30\n",
    "        feedback.append(\"âœ“ Model is fitted\")\n",
    "    \n",
    "    # Check accuracy if test data exists (30 points)\n",
    "    if all(var in globals_dict for var in ['X_test', 'y_test']):\n",
    "        try:\n",
    "            accuracy = model.score(globals_dict['X_test'], globals_dict['y_test'])\n",
    "            if accuracy > 0.6:\n",
    "                score += 30\n",
    "                feedback.append(f\"âœ“ Good accuracy: {accuracy:.2%}\")\n",
    "            else:\n",
    "                score += 15\n",
    "                feedback.append(f\"âš  Low accuracy: {accuracy:.2%}\")\n",
    "        except:\n",
    "            feedback.append(\"âœ— Could not evaluate model accuracy\")\n",
    "    \n",
    "    # Display feedback\n",
    "    for item in feedback:\n",
    "        print(item)\n",
    "    \n",
    "    return score > 50, score\n",
    "\n",
    "# Add this comprehensive rule\n",
    "validator.add_scoring_rule(\n",
    "    name=\"Comprehensive Model Check\",\n",
    "    weight=0.0,  # Set to 0 to not affect current grading\n",
    "    checker=comprehensive_model_check,\n",
    "    description=\"Advanced model validation with multiple criteria\"\n",
    ")\n",
    "\n",
    "# Test the comprehensive check\n",
    "success, score = validator.validate_with_score(\"Comprehensive Model Check\", globals_dict=globals())\n",
    "print(f\"\\nComprehensive check score: {score}/100\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "The auto-grading system provides:\n",
    "- **Weighted scoring rules** for flexible grading schemes\n",
    "- **Partial credit** support for nuanced evaluation\n",
    "- **Custom validation functions** for complex checks\n",
    "- **Visual feedback** with color-coded results\n",
    "- **Grade calculation** with letter grades\n",
    "- **Export capabilities** for record keeping\n",
    "- **History tracking** for audit trails"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}